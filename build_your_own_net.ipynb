{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train_a_Network.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RodericGuigoCorominas/datascience/blob/main/build_your_own_net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjX6Fjgt0n93",
        "outputId": "51028218-a6b6-413a-f778-65dda7116835"
      },
      "source": [
        "###DO NOT EDIT THIS CODE\n",
        "################################################################################################################################\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')\n",
        "%cd '/content/gdrive/MyDrive/Easy Nets/'\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#GPU or CPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#Loss Function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "#Training Function\n",
        "def train(epochs, model, dataloader, testloader, optimizer, loss_function):\n",
        "  for epoch in range(epochs):\n",
        "    loss_epoch = np.array([])\n",
        "    correct, total = (0,0)\n",
        "    for data, labels in dataloader:\n",
        "      input_data = Variable(data).to(device)\n",
        "      labels = labels.to(device)\n",
        "      predict = model(input_data)\n",
        "      loss = loss_function(predict, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      loss_epoch = np.append(loss_epoch, loss.item())\n",
        "    \n",
        "    for data, labels in testloader:\n",
        "      input_data = Variable(data).to(device)\n",
        "      labels = labels.to(device)\n",
        "      predict = model(input_data)\n",
        "      for i, out in enumerate(predict):\n",
        "        pred = torch.argmax(out)\n",
        "        if pred == labels[i]:\n",
        "          correct+=1\n",
        "        total+=1\n",
        "\n",
        "    accuracy = correct/total    \n",
        "  \n",
        "    print('epoch [{}/{}], training loss:{:.4f}, accuracy:{:.4f}'.format(epoch+1, epochs, np.mean(loss_epoch), accuracy))\n",
        "################################################################################################################################\n",
        "\n",
        "def num_params(model):\n",
        "  return sum([p.numel() for p in model.parameters()])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n",
            "/content/gdrive/.shortcut-targets-by-id/1IrVCGk9CH8ZYDWEH4w96PenFpHYJx6Cs/Easy Nets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeRdw4N1ZfM"
      },
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "Available datasets are: MNIST, CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVrE2cdmVXY"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbgwSn41Fyy"
      },
      "source": [
        "# download and load data\n",
        "batch_size = 512\n",
        "\n",
        "# download and transform train dataset\n",
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transforms.Compose([\n",
        "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                ])), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# download and transform test dataset\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
        "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                          ])), batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94iP5A31mYQN"
      },
      "source": [
        "## SVHN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzzuPUXxmRQ4",
        "outputId": "63e05a61-9456-4690-d488-5e7b94ac7a20"
      },
      "source": [
        "# download and load data\n",
        "batch_size = 512\n",
        "\n",
        "# download and transform train dataset\n",
        "train_dataset = datasets.SVHN(root='/content/gdrive/.shortcut-targets-by-id/1IrVCGk9CH8ZYDWEH4w96PenFpHYJx6Cs/Easy Nets', \n",
        "                                                         split='train', transform=None, download=True)\n",
        "train_dataset = [(transforms.ToTensor()(img[0]), img[1]) for img in train_dataset]\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# download and transform test dataset\n",
        "test_dataset = datasets.SVHN(root='/content/gdrive/.shortcut-targets-by-id/1IrVCGk9CH8ZYDWEH4w96PenFpHYJx6Cs/Easy Nets',\n",
        "                                                        split='test', transform=None, download=True)\n",
        "test_dataset = [(transforms.ToTensor()(img[0]), img[1]) for img in test_dataset]\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using downloaded and verified file: /content/gdrive/.shortcut-targets-by-id/1IrVCGk9CH8ZYDWEH4w96PenFpHYJx6Cs/Easy Nets/train_32x32.mat\n",
            "Using downloaded and verified file: /content/gdrive/.shortcut-targets-by-id/1IrVCGk9CH8ZYDWEH4w96PenFpHYJx6Cs/Easy Nets/test_32x32.mat\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0A5f-elnONB",
        "outputId": "6972abf7-0cec-42af-cf75-b7c2bd9eed46"
      },
      "source": [
        "### Test Dimensions\n",
        "iterable = iter(train_loader)\n",
        "print(iterable.next()[0].shape)\n",
        "print(iterable.next()[1].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([512, 3, 32, 32])\n",
            "torch.Size([512])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTE0ttEd2CX9"
      },
      "source": [
        "# **Build Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UHuwEOfO0s"
      },
      "source": [
        "# neural network\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    ###Add Layers\n",
        "\n",
        "  def forward(self, x):\n",
        "    ###Feed Forward\n",
        "    return(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWVImu4XrYAn"
      },
      "source": [
        "# create model object\n",
        "model = CNN().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BId5KEzASWc"
      },
      "source": [
        "# **Define Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hlg1WpBFAXAc"
      },
      "source": [
        "learning_rate = 10e-3\n",
        "weight_decay = 10e-5\n",
        "n_epochs = 10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQUAZDKI19M5"
      },
      "source": [
        "# **Chose Optimizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l01-XG6Y2B2O"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6vNthxXAbHg"
      },
      "source": [
        "# **Train and Validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgFfMHXeAgca",
        "outputId": "bb4d676e-bf6e-4a22-fdb2-b01fce2d39ea"
      },
      "source": [
        "train(n_epochs, model, train_loader, test_loader, optimizer, loss_function)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch [1/10], training loss:1.3661, accuracy:0.8138\n",
            "epoch [2/10], training loss:0.4834, accuracy:0.8557\n",
            "epoch [3/10], training loss:0.3918, accuracy:0.8768\n",
            "epoch [4/10], training loss:0.3531, accuracy:0.8861\n",
            "epoch [5/10], training loss:0.3224, accuracy:0.8871\n",
            "epoch [6/10], training loss:0.3049, accuracy:0.8928\n",
            "epoch [7/10], training loss:0.2869, accuracy:0.9020\n",
            "epoch [8/10], training loss:0.2736, accuracy:0.8940\n",
            "epoch [9/10], training loss:0.2630, accuracy:0.9011\n",
            "epoch [10/10], training loss:0.2583, accuracy:0.9098\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8_H8B3UfJAP"
      },
      "source": [
        "# **No Batch Normalization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjfBURSHAjdO"
      },
      "source": [
        "# neural network\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(1, 16, kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "    self.conv5 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    self.avg = nn.AdaptiveAvgPool2d(4)\n",
        "    self.fc = nn.Linear(1024,10)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.conv1(x))\n",
        "    x = torch.relu(self.conv2(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = torch.relu(self.conv3(x))\n",
        "    x = torch.relu(self.conv4(x))\n",
        "    x = torch.relu(self.conv5(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = self.avg(x)\n",
        "    x = x.view(-1,1024)\n",
        "    x = self.fc(x)\n",
        "    x = self.softmax(x)\n",
        "    return(x)\n",
        "\n",
        "model = CNN().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVgkkg2qe9Ee"
      },
      "source": [
        "# **Best Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX_BxQc01T-U"
      },
      "source": [
        "# neural network\n",
        "class CNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CNN, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(3, 16, kernel_size=5, padding=2)\n",
        "    self.conv2 = nn.Conv2d(16, 16, kernel_size=3, padding=1)\n",
        "    self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.bn1 = nn.BatchNorm2d(16)\n",
        "    self.conv3 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "    self.conv4 = nn.Conv2d(32, 32, kernel_size=3, padding=1)\n",
        "    self.conv5 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "    self.bn2 = nn.BatchNorm2d(64)\n",
        "    self.avg = nn.AdaptiveAvgPool2d(4)\n",
        "    self.fc = nn.Linear(1024,10)\n",
        "    self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = torch.relu(self.conv1(x))\n",
        "    x = torch.relu(self.conv2(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = self.bn1(x)\n",
        "    x = torch.relu(self.conv3(x))\n",
        "    x = torch.relu(self.conv4(x))\n",
        "    x = torch.relu(self.conv5(x))\n",
        "    x = self.maxpool(x)\n",
        "    x = self.bn2(x)\n",
        "    x = self.avg(x)\n",
        "    x = x.view(-1,1024)\n",
        "    x = self.fc(x)\n",
        "    x = self.softmax(x)\n",
        "    return(x)\n",
        "\n",
        "model = CNN().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}