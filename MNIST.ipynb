{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST ",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasyaa04/datascience/blob/main/MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjX6Fjgt0n93"
      },
      "source": [
        "###DO NOT EDIT THIS CODE\n",
        "################################################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# GPUs are 3x faster than CPU. Better to use if it is available \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define Loss Function\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# This function returns the number of parameters in the model\n",
        "def num_params(model):\n",
        "  return sum([p.numel() for p in model.parameters()])\n",
        "\n",
        "# Define a Training Function. This function will: compute the forward pass, backpropagate,\n",
        "# update the weights, and repeat the steps for a given number of epochs. At each epoch, \n",
        "# it will output the training loss and test loss at every step\n",
        "def train(epochs, model, trainloader, testloader, optimizer, loss_function):\n",
        "  for epoch in range(epochs):\n",
        "    loss_epoch = np.array([])\n",
        "    train_correct, train_total = 0, 0\n",
        "    test_correct, test_total = 0, 0\n",
        "\n",
        "    for data, labels in trainloader:\n",
        "      # convert into GPU objects if needed\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      predict = model(input_data)\n",
        "      \n",
        "      # backward pass\n",
        "      loss = loss_function(predict, labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "\n",
        "      # update parameters (weights and biases)\n",
        "      optimizer.step()\n",
        "\n",
        "      # store progress\n",
        "      loss_epoch = np.append(loss_epoch, loss.item())\n",
        "\n",
        "    # evaluate test accuracy\n",
        "    for data, labels in testloader:\n",
        "      input_data = data.to(device)\n",
        "      labels = labels.to(device)\n",
        "      predict = model(input_data)\n",
        "      for i, out in enumerate(predict):\n",
        "        pred = torch.argmax(out)\n",
        "        if pred == labels[i]:\n",
        "          test_correct+=1\n",
        "        test_total+=1\n",
        "\n",
        "    test_accuracy = test_correct/test_total    \n",
        "  \n",
        "    print('epoch [{}/{}], training loss:{:.4f}, test accuracy:{:.4f}'.format(epoch+1, epochs, np.mean(loss_epoch), test_accuracy))\n",
        "################################################################################################################################"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pbeRdw4N1ZfM"
      },
      "source": [
        "# **Load Dataset**\n",
        "\n",
        "Available datasets are: MNIST, CIFAR10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iVrE2cdmVXY"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWbgwSn41Fyy"
      },
      "source": [
        "# download and load data\n",
        "batch_size = 512\n",
        "\n",
        "# download and transform train dataset\n",
        "train_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=True, transform=transforms.Compose([\n",
        "                                                transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                ])), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# download and transform test dataset\n",
        "test_loader = torch.utils.data.DataLoader(datasets.MNIST('./mnist_data', download=True, train=False, transform=transforms.Compose([\n",
        "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
        "                                                              transforms.Normalize((0.1307,), (0.3081,)) # normalize inputs\n",
        "                                                          ])), batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# it is a good idea to take a look at the data. Here we see it is a 28x28 grayscale image\n",
        "for data, labels in train_loader:\n",
        "  print(data[0].size())\n",
        "  break"
      ],
      "metadata": {
        "id": "nn-luYwzxz4O",
        "outputId": "58305ac4-6afb-493b-92b7-9d12257e727b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTE0ttEd2CX9"
      },
      "source": [
        "# **Build a Network and Define Hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8UHuwEOfO0s"
      },
      "source": [
        "########################\n",
        "####                #### \n",
        "#### EDIT THIS CELL ####\n",
        "####                ####\n",
        "########################\n",
        "\n",
        "\n",
        "learning_rate = 17e-3\n",
        "weight_decay = 10e-5\n",
        "n_epochs = 10\n",
        "\n",
        "# neural network\n",
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    ### Define Layers\n",
        "    self.conv1 = nn.Conv2d(1, 3, 3, padding=1)\n",
        "    self.fc1 = nn.Linear(2352, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.conv1(x)\n",
        "    x = F.relu(x)\n",
        "    x = torch.flatten(x, 1)\n",
        "    x = self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.sigmoid(x)\n",
        "    x = F.relu(x)\n",
        "    return x\n",
        "  \n",
        "# Every time you edit the neural network, you'll have to update this cell\n",
        "# Create model object\n",
        "model = NeuralNetwork().to(device)\n",
        "\n",
        "# Loads Adam optimizer, which implements a version of gradient descent\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the structure of your network\n",
        "print(model)\n",
        "\n",
        "# apply your model to a single input. This helps check that \n",
        "# the dimensions are correct\n",
        "model(torch.rand(1,1,28,28, device=device))"
      ],
      "metadata": {
        "id": "eccjx7zG0Mbk",
        "outputId": "9b8e932a-a865-4c5b-ae42-89785515ed7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NeuralNetwork(\n",
            "  (conv1): Conv2d(1, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (fc1): Linear(in_features=2352, out_features=10, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5002, 0.5295, 0.5027, 0.5013, 0.5019, 0.5000, 0.5487, 0.5000, 0.5000,\n",
              "         0.5444]], device='cuda:0', grad_fn=<ReluBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6vNthxXAbHg"
      },
      "source": [
        "# **Train and Validate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgFfMHXeAgca",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ad66a7f-822d-4ce4-8157-be940855a7b6"
      },
      "source": [
        "train(n_epochs, model, train_loader, test_loader, optimizer, loss_function)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch [1/10], training loss:1.8895, test accuracy:0.9546\n",
            "epoch [2/10], training loss:1.8897, test accuracy:0.9530\n",
            "epoch [3/10], training loss:1.8892, test accuracy:0.9564\n",
            "epoch [4/10], training loss:1.8886, test accuracy:0.9524\n",
            "epoch [5/10], training loss:1.8884, test accuracy:0.9557\n",
            "epoch [6/10], training loss:1.8889, test accuracy:0.9513\n",
            "epoch [7/10], training loss:1.8884, test accuracy:0.9562\n",
            "epoch [8/10], training loss:1.8886, test accuracy:0.9566\n",
            "epoch [9/10], training loss:1.8886, test accuracy:0.9529\n",
            "epoch [10/10], training loss:1.8903, test accuracy:0.9578\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "llQ_ONSbG6_P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}